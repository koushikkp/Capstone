{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koushik/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv(\"/home/koushik/Project/train/labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bluetick'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.iloc[3]['breed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "\n",
    "for j in range(len(a)):\n",
    "    b=a.iloc[j]['breed']\n",
    "    labels.append(b)\n",
    "    \n",
    "\n",
    "unique_labels = list(set(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/koushik/Project/imgs'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/home/koushik/Project/imgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in a.iterrows():\n",
    "    if not tf.gfile.Exists('/home/koushik/Project/imgs/%s'%(row['breed'])):\n",
    "        tf.gfile.MkDir('/home/koushik/Project/imgs/%s'%(row['breed']))\n",
    "#     print('train/%s.jpg'%(row['id']),'dog_train/%s'%(row['breed']))\n",
    "    tf.gfile.Copy('/home/koushik/Project/train/%s.jpg'%(row['id']),'/home/koushik/Project/imgs/%s/%s.jpg'%(row['breed'],row['id']),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ibizan_hound\n",
      "Done afghan_hound\n",
      "Done labrador_retriever\n",
      "Done soft-coated_wheaten_terrier\n",
      "Done tibetan_terrier\n",
      "Done schipperke\n",
      "Done chow\n",
      "Done kelpie\n",
      "Done saluki\n",
      "Done standard_poodle\n",
      "Done toy_terrier\n",
      "Done border_collie\n",
      "Done toy_poodle\n",
      "Done affenpinscher\n",
      "Done weimaraner\n",
      "Done boxer\n",
      "Done basset\n",
      "Done mexican_hairless\n",
      "Done dandie_dinmont\n",
      "Done english_springer\n",
      "Done entlebucher\n",
      "Done old_english_sheepdog\n",
      "Done flat-coated_retriever\n",
      "Done english_foxhound\n",
      "Done gordon_setter\n",
      "Done walker_hound\n",
      "Done irish_setter\n",
      "Done rhodesian_ridgeback\n",
      "Done komondor\n",
      "Done beagle\n",
      "Done standard_schnauzer\n",
      "Done silky_terrier\n",
      "Done west_highland_white_terrier\n",
      "Done chesapeake_bay_retriever\n",
      "Done bernese_mountain_dog\n",
      "Done wire-haired_fox_terrier\n",
      "Done african_hunting_dog\n",
      "Done basenji\n",
      "Done giant_schnauzer\n",
      "Done redbone\n",
      "Done lhasa\n",
      "Done collie\n",
      "Done dingo\n",
      "Done pembroke\n",
      "Done pug\n",
      "Done border_terrier\n",
      "Done samoyed\n",
      "Done rottweiler\n",
      "Done kerry_blue_terrier\n",
      "Done cairn\n",
      "Done whippet\n",
      "Done great_dane\n",
      "Done maltese_dog\n",
      "Done vizsla\n",
      "Done shetland_sheepdog\n",
      "Done great_pyrenees\n",
      "Done sussex_spaniel\n",
      "Done japanese_spaniel\n",
      "Done eskimo_dog\n",
      "Done greater_swiss_mountain_dog\n",
      "Done blenheim_spaniel\n",
      "Done pomeranian\n",
      "Done golden_retriever\n",
      "Done briard\n",
      "Done clumber\n",
      "Done norwich_terrier\n",
      "Done newfoundland\n",
      "Done french_bulldog\n",
      "Done miniature_schnauzer\n",
      "Done papillon\n",
      "Done english_setter\n",
      "Done black-and-tan_coonhound\n",
      "Done german_short-haired_pointer\n",
      "Done miniature_pinscher\n",
      "Done sealyham_terrier\n",
      "Done norfolk_terrier\n",
      "Done pekinese\n",
      "Done tibetan_mastiff\n",
      "Done yorkshire_terrier\n",
      "Done american_staffordshire_terrier\n",
      "Done lakeland_terrier\n",
      "Done scotch_terrier\n",
      "Done malamute\n",
      "Done bloodhound\n",
      "Done welsh_springer_spaniel\n",
      "Done cocker_spaniel\n",
      "Done curly-coated_retriever\n",
      "Done siberian_husky\n",
      "Done bedlington_terrier\n",
      "Done irish_wolfhound\n",
      "Done appenzeller\n",
      "Done shih-tzu\n",
      "Done saint_bernard\n",
      "Done otterhound\n",
      "Done bouvier_des_flandres\n",
      "Done bluetick\n",
      "Done dhole\n",
      "Done australian_terrier\n",
      "Done staffordshire_bullterrier\n",
      "Done brabancon_griffon\n",
      "Done borzoi\n",
      "Done keeshond\n",
      "Done kuvasz\n",
      "Done miniature_poodle\n",
      "Done irish_water_spaniel\n",
      "Done doberman\n",
      "Done malinois\n",
      "Done chihuahua\n",
      "Done scottish_deerhound\n",
      "Done irish_terrier\n",
      "Done boston_bull\n",
      "Done bull_mastiff\n",
      "Done airedale\n",
      "Done cardigan\n",
      "Done italian_greyhound\n",
      "Done german_shepherd\n",
      "Done leonberg\n",
      "Done groenendael\n",
      "Done norwegian_elkhound\n",
      "Done brittany_spaniel\n"
     ]
    }
   ],
   "source": [
    "result_array = np.empty((0, 2049))\n",
    "location = '/home/koushik/Project/bottleneck'\n",
    "folder_list = listdir(location)\n",
    "for folder in folder_list:\n",
    "   folder_loc = location+'/'+folder\n",
    "   file_list = os.listdir(folder_loc)\n",
    "   \n",
    "   for i in file_list:\n",
    "      \n",
    "       \n",
    "       file_list_loc = folder_loc+'/'+i\n",
    "       data1 = pd.read_csv(file_list_loc, header= None )\n",
    "       data1['labels']=str(folder)\n",
    "       result_array = np.append(result_array,data1,axis= 0)\n",
    "        \n",
    "       \n",
    "       \n",
    "   print('Done '+str(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data=data1.iloc[0,:-1]\n",
    "Y_data=data1.iloc[0,2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.191749\n",
       "1        0.931048\n",
       "2       0.0632289\n",
       "3       0.0295059\n",
       "4        0.378287\n",
       "5        0.381569\n",
       "6        0.509593\n",
       "7        0.347193\n",
       "8        0.188065\n",
       "9        0.115995\n",
       "10       0.643584\n",
       "11       0.257208\n",
       "12       0.156395\n",
       "13       0.365272\n",
       "14       0.253867\n",
       "15       0.389141\n",
       "16      0.0564845\n",
       "17       0.110858\n",
       "18      0.0992265\n",
       "19       0.220992\n",
       "20        0.21997\n",
       "21      0.0590616\n",
       "22      0.0392087\n",
       "23       0.115753\n",
       "24       0.311962\n",
       "25       0.206602\n",
       "26       0.662101\n",
       "27       0.437275\n",
       "28      0.0525824\n",
       "29       0.360073\n",
       "          ...    \n",
       "2018     0.158689\n",
       "2019    0.0594824\n",
       "2020     0.411323\n",
       "2021    0.0651606\n",
       "2022    0.0430771\n",
       "2023     0.165217\n",
       "2024     0.717913\n",
       "2025      1.38671\n",
       "2026     0.135714\n",
       "2027    0.0901842\n",
       "2028    0.0863477\n",
       "2029    0.0989096\n",
       "2030    0.0212451\n",
       "2031     0.737653\n",
       "2032     0.570433\n",
       "2033     0.816344\n",
       "2034      0.29913\n",
       "2035    0.0266645\n",
       "2036     0.681565\n",
       "2037    0.0838059\n",
       "2038     0.124918\n",
       "2039    0.0552494\n",
       "2040     0.410694\n",
       "2041     0.519017\n",
       "2042    0.0307063\n",
       "2043     0.325795\n",
       "2044    0.0694224\n",
       "2045      0.38681\n",
       "2046     0.633806\n",
       "2047     0.457404\n",
       "Name: 0, Length: 2048, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brittany_spaniel'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 2049)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.191749</td>\n",
       "      <td>0.931048</td>\n",
       "      <td>0.063229</td>\n",
       "      <td>0.029506</td>\n",
       "      <td>0.378287</td>\n",
       "      <td>0.381569</td>\n",
       "      <td>0.509593</td>\n",
       "      <td>0.347193</td>\n",
       "      <td>0.188065</td>\n",
       "      <td>0.115995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.410694</td>\n",
       "      <td>0.519017</td>\n",
       "      <td>0.030706</td>\n",
       "      <td>0.325795</td>\n",
       "      <td>0.069422</td>\n",
       "      <td>0.38681</td>\n",
       "      <td>0.633806</td>\n",
       "      <td>0.457404</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2049 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.191749  0.931048  0.063229  0.029506  0.378287  0.381569  0.509593   \n",
       "\n",
       "          7         8         9        ...             2039      2040  \\\n",
       "0  0.347193  0.188065  0.115995        ...         0.055249  0.410694   \n",
       "\n",
       "       2041      2042      2043      2044     2045      2046      2047  \\\n",
       "0  0.519017  0.030706  0.325795  0.069422  0.38681  0.633806  0.457404   \n",
       "\n",
       "             labels  \n",
       "0  brittany_spaniel  \n",
       "\n",
       "[1 rows x 2049 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koushik/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 1024)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mix = pd.read_csv(\"/home/koushik/Downloads/feature.csv\") ### I saved the result_array as csv file so that I can access it from my personal laptop and work it in home \n",
    "#### So I accessed the result_array again in my personal laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>2048</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.268131</td>\n",
       "      <td>0.314545</td>\n",
       "      <td>0.099742</td>\n",
       "      <td>0.368813</td>\n",
       "      <td>0.496304</td>\n",
       "      <td>0.127623</td>\n",
       "      <td>0.197701</td>\n",
       "      <td>0.242251</td>\n",
       "      <td>0.240683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313441</td>\n",
       "      <td>0.051260</td>\n",
       "      <td>0.339381</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.036992</td>\n",
       "      <td>0.464850</td>\n",
       "      <td>0.404165</td>\n",
       "      <td>0.896529</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.535600</td>\n",
       "      <td>0.428560</td>\n",
       "      <td>0.341043</td>\n",
       "      <td>0.183151</td>\n",
       "      <td>0.419171</td>\n",
       "      <td>0.394426</td>\n",
       "      <td>0.647834</td>\n",
       "      <td>0.458548</td>\n",
       "      <td>0.191399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409865</td>\n",
       "      <td>0.380054</td>\n",
       "      <td>0.102255</td>\n",
       "      <td>0.192480</td>\n",
       "      <td>0.321079</td>\n",
       "      <td>0.721681</td>\n",
       "      <td>0.238019</td>\n",
       "      <td>0.487049</td>\n",
       "      <td>0.145839</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.411610</td>\n",
       "      <td>0.346167</td>\n",
       "      <td>0.244413</td>\n",
       "      <td>0.137211</td>\n",
       "      <td>0.465395</td>\n",
       "      <td>0.180839</td>\n",
       "      <td>0.324190</td>\n",
       "      <td>0.394368</td>\n",
       "      <td>0.409295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368454</td>\n",
       "      <td>0.553193</td>\n",
       "      <td>1.081795</td>\n",
       "      <td>0.272835</td>\n",
       "      <td>0.298575</td>\n",
       "      <td>1.154528</td>\n",
       "      <td>0.635267</td>\n",
       "      <td>0.131573</td>\n",
       "      <td>0.061658</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.388489</td>\n",
       "      <td>0.429210</td>\n",
       "      <td>0.204412</td>\n",
       "      <td>0.227942</td>\n",
       "      <td>0.096363</td>\n",
       "      <td>0.562920</td>\n",
       "      <td>0.308009</td>\n",
       "      <td>0.209543</td>\n",
       "      <td>0.299920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545821</td>\n",
       "      <td>0.413907</td>\n",
       "      <td>0.363741</td>\n",
       "      <td>0.131718</td>\n",
       "      <td>0.365889</td>\n",
       "      <td>0.248360</td>\n",
       "      <td>0.077544</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>0.663523</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.336401</td>\n",
       "      <td>0.209270</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.294768</td>\n",
       "      <td>0.138025</td>\n",
       "      <td>0.080287</td>\n",
       "      <td>0.067682</td>\n",
       "      <td>0.252708</td>\n",
       "      <td>0.529406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393116</td>\n",
       "      <td>0.055445</td>\n",
       "      <td>0.051030</td>\n",
       "      <td>0.041980</td>\n",
       "      <td>0.222905</td>\n",
       "      <td>0.941616</td>\n",
       "      <td>0.091477</td>\n",
       "      <td>0.985495</td>\n",
       "      <td>0.258923</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.179261</td>\n",
       "      <td>0.179760</td>\n",
       "      <td>0.133865</td>\n",
       "      <td>0.224384</td>\n",
       "      <td>0.227002</td>\n",
       "      <td>0.330131</td>\n",
       "      <td>0.257557</td>\n",
       "      <td>0.125157</td>\n",
       "      <td>0.399683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301558</td>\n",
       "      <td>0.029274</td>\n",
       "      <td>0.232163</td>\n",
       "      <td>0.019113</td>\n",
       "      <td>0.798135</td>\n",
       "      <td>0.920266</td>\n",
       "      <td>0.678088</td>\n",
       "      <td>0.206816</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.241880</td>\n",
       "      <td>0.165525</td>\n",
       "      <td>0.069135</td>\n",
       "      <td>0.424274</td>\n",
       "      <td>0.231150</td>\n",
       "      <td>0.137054</td>\n",
       "      <td>0.103857</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>0.063510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085624</td>\n",
       "      <td>0.496469</td>\n",
       "      <td>0.406844</td>\n",
       "      <td>0.368322</td>\n",
       "      <td>0.115259</td>\n",
       "      <td>0.570887</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>0.347292</td>\n",
       "      <td>0.024614</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.142026</td>\n",
       "      <td>0.348606</td>\n",
       "      <td>0.247162</td>\n",
       "      <td>0.341521</td>\n",
       "      <td>0.283595</td>\n",
       "      <td>0.144450</td>\n",
       "      <td>0.137762</td>\n",
       "      <td>0.188331</td>\n",
       "      <td>0.476523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089191</td>\n",
       "      <td>0.010731</td>\n",
       "      <td>0.272730</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>0.086470</td>\n",
       "      <td>1.165994</td>\n",
       "      <td>0.387214</td>\n",
       "      <td>0.908632</td>\n",
       "      <td>0.079460</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.114619</td>\n",
       "      <td>1.468325</td>\n",
       "      <td>0.127875</td>\n",
       "      <td>0.124827</td>\n",
       "      <td>0.282348</td>\n",
       "      <td>0.441101</td>\n",
       "      <td>0.761734</td>\n",
       "      <td>0.304396</td>\n",
       "      <td>0.313244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379911</td>\n",
       "      <td>0.006663</td>\n",
       "      <td>0.382955</td>\n",
       "      <td>0.295957</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.334304</td>\n",
       "      <td>0.409749</td>\n",
       "      <td>0.943929</td>\n",
       "      <td>0.087351</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.089565</td>\n",
       "      <td>0.288995</td>\n",
       "      <td>0.011433</td>\n",
       "      <td>0.243081</td>\n",
       "      <td>0.275586</td>\n",
       "      <td>0.239540</td>\n",
       "      <td>0.096828</td>\n",
       "      <td>0.040528</td>\n",
       "      <td>0.026333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104241</td>\n",
       "      <td>0.035879</td>\n",
       "      <td>0.646680</td>\n",
       "      <td>0.307638</td>\n",
       "      <td>0.445135</td>\n",
       "      <td>0.616339</td>\n",
       "      <td>0.508509</td>\n",
       "      <td>0.543701</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.341505</td>\n",
       "      <td>0.267562</td>\n",
       "      <td>0.064096</td>\n",
       "      <td>0.231461</td>\n",
       "      <td>0.137724</td>\n",
       "      <td>0.104054</td>\n",
       "      <td>0.192591</td>\n",
       "      <td>0.099019</td>\n",
       "      <td>0.279889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266945</td>\n",
       "      <td>0.133081</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.094724</td>\n",
       "      <td>0.054535</td>\n",
       "      <td>0.528031</td>\n",
       "      <td>0.725547</td>\n",
       "      <td>0.168303</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.250340</td>\n",
       "      <td>0.332290</td>\n",
       "      <td>0.055295</td>\n",
       "      <td>0.226765</td>\n",
       "      <td>0.351287</td>\n",
       "      <td>0.168232</td>\n",
       "      <td>0.318178</td>\n",
       "      <td>0.135355</td>\n",
       "      <td>0.320461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675125</td>\n",
       "      <td>0.067552</td>\n",
       "      <td>0.408428</td>\n",
       "      <td>0.306361</td>\n",
       "      <td>0.019928</td>\n",
       "      <td>0.695267</td>\n",
       "      <td>0.128020</td>\n",
       "      <td>0.784032</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.434534</td>\n",
       "      <td>0.325708</td>\n",
       "      <td>0.200167</td>\n",
       "      <td>0.189003</td>\n",
       "      <td>0.144660</td>\n",
       "      <td>0.102106</td>\n",
       "      <td>0.136878</td>\n",
       "      <td>0.183860</td>\n",
       "      <td>0.193843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057799</td>\n",
       "      <td>0.023907</td>\n",
       "      <td>0.090878</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>0.110228</td>\n",
       "      <td>0.978029</td>\n",
       "      <td>0.549964</td>\n",
       "      <td>0.171643</td>\n",
       "      <td>0.156570</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.273670</td>\n",
       "      <td>0.885893</td>\n",
       "      <td>0.194608</td>\n",
       "      <td>0.132516</td>\n",
       "      <td>0.378691</td>\n",
       "      <td>0.115035</td>\n",
       "      <td>0.126360</td>\n",
       "      <td>0.210387</td>\n",
       "      <td>0.287135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357561</td>\n",
       "      <td>0.036834</td>\n",
       "      <td>0.578533</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.014387</td>\n",
       "      <td>0.025846</td>\n",
       "      <td>0.431706</td>\n",
       "      <td>0.591010</td>\n",
       "      <td>0.636501</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.137402</td>\n",
       "      <td>0.665896</td>\n",
       "      <td>0.247767</td>\n",
       "      <td>0.368408</td>\n",
       "      <td>0.293185</td>\n",
       "      <td>0.213098</td>\n",
       "      <td>0.238626</td>\n",
       "      <td>0.131047</td>\n",
       "      <td>0.387308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506970</td>\n",
       "      <td>0.458128</td>\n",
       "      <td>0.198881</td>\n",
       "      <td>0.051571</td>\n",
       "      <td>0.548891</td>\n",
       "      <td>0.424909</td>\n",
       "      <td>0.211861</td>\n",
       "      <td>0.934903</td>\n",
       "      <td>0.019817</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.431510</td>\n",
       "      <td>0.605591</td>\n",
       "      <td>0.071247</td>\n",
       "      <td>0.403741</td>\n",
       "      <td>0.445838</td>\n",
       "      <td>0.282891</td>\n",
       "      <td>0.546789</td>\n",
       "      <td>0.397571</td>\n",
       "      <td>0.282477</td>\n",
       "      <td>...</td>\n",
       "      <td>1.295817</td>\n",
       "      <td>0.185884</td>\n",
       "      <td>0.283274</td>\n",
       "      <td>0.126573</td>\n",
       "      <td>0.105886</td>\n",
       "      <td>0.612299</td>\n",
       "      <td>0.240265</td>\n",
       "      <td>1.128039</td>\n",
       "      <td>1.200978</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.123874</td>\n",
       "      <td>0.491449</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>0.352269</td>\n",
       "      <td>0.133436</td>\n",
       "      <td>0.058631</td>\n",
       "      <td>0.106053</td>\n",
       "      <td>0.066626</td>\n",
       "      <td>0.209183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370686</td>\n",
       "      <td>0.318205</td>\n",
       "      <td>0.679017</td>\n",
       "      <td>0.548500</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.393311</td>\n",
       "      <td>0.440081</td>\n",
       "      <td>0.569622</td>\n",
       "      <td>0.304598</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.405292</td>\n",
       "      <td>0.367609</td>\n",
       "      <td>0.029684</td>\n",
       "      <td>0.350399</td>\n",
       "      <td>0.474873</td>\n",
       "      <td>0.133523</td>\n",
       "      <td>0.130907</td>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.338498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598368</td>\n",
       "      <td>0.068239</td>\n",
       "      <td>0.180261</td>\n",
       "      <td>0.054458</td>\n",
       "      <td>0.131565</td>\n",
       "      <td>0.583949</td>\n",
       "      <td>0.918237</td>\n",
       "      <td>0.775041</td>\n",
       "      <td>0.018151</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.191915</td>\n",
       "      <td>0.805106</td>\n",
       "      <td>0.216228</td>\n",
       "      <td>0.234681</td>\n",
       "      <td>0.425426</td>\n",
       "      <td>0.129702</td>\n",
       "      <td>0.412327</td>\n",
       "      <td>0.274994</td>\n",
       "      <td>0.201805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>0.027381</td>\n",
       "      <td>0.189027</td>\n",
       "      <td>0.041245</td>\n",
       "      <td>0.325206</td>\n",
       "      <td>0.783235</td>\n",
       "      <td>0.442924</td>\n",
       "      <td>0.627115</td>\n",
       "      <td>0.024688</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.366920</td>\n",
       "      <td>0.334553</td>\n",
       "      <td>0.146133</td>\n",
       "      <td>0.209513</td>\n",
       "      <td>0.211596</td>\n",
       "      <td>0.136278</td>\n",
       "      <td>0.099392</td>\n",
       "      <td>0.030709</td>\n",
       "      <td>0.234046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.032111</td>\n",
       "      <td>0.372326</td>\n",
       "      <td>0.102607</td>\n",
       "      <td>0.112899</td>\n",
       "      <td>0.725864</td>\n",
       "      <td>0.803593</td>\n",
       "      <td>0.273651</td>\n",
       "      <td>0.013946</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.124299</td>\n",
       "      <td>0.924571</td>\n",
       "      <td>0.071838</td>\n",
       "      <td>0.283004</td>\n",
       "      <td>0.211197</td>\n",
       "      <td>0.110888</td>\n",
       "      <td>0.270058</td>\n",
       "      <td>0.128013</td>\n",
       "      <td>0.206459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205755</td>\n",
       "      <td>0.102496</td>\n",
       "      <td>0.908816</td>\n",
       "      <td>0.080889</td>\n",
       "      <td>0.300991</td>\n",
       "      <td>0.801323</td>\n",
       "      <td>0.368663</td>\n",
       "      <td>0.626947</td>\n",
       "      <td>0.083503</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.532367</td>\n",
       "      <td>0.294329</td>\n",
       "      <td>0.071301</td>\n",
       "      <td>0.305470</td>\n",
       "      <td>0.483052</td>\n",
       "      <td>0.117039</td>\n",
       "      <td>0.208858</td>\n",
       "      <td>0.099556</td>\n",
       "      <td>0.181566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634013</td>\n",
       "      <td>0.292816</td>\n",
       "      <td>0.151513</td>\n",
       "      <td>0.070826</td>\n",
       "      <td>0.115341</td>\n",
       "      <td>0.796879</td>\n",
       "      <td>0.466404</td>\n",
       "      <td>0.213822</td>\n",
       "      <td>0.285758</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.179497</td>\n",
       "      <td>0.245316</td>\n",
       "      <td>0.090061</td>\n",
       "      <td>0.194826</td>\n",
       "      <td>0.117493</td>\n",
       "      <td>0.125660</td>\n",
       "      <td>0.046893</td>\n",
       "      <td>0.194317</td>\n",
       "      <td>0.530851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347062</td>\n",
       "      <td>0.069267</td>\n",
       "      <td>0.035161</td>\n",
       "      <td>0.052164</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.597529</td>\n",
       "      <td>0.189040</td>\n",
       "      <td>0.842388</td>\n",
       "      <td>0.289425</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.324868</td>\n",
       "      <td>0.865574</td>\n",
       "      <td>0.044108</td>\n",
       "      <td>0.205583</td>\n",
       "      <td>0.496671</td>\n",
       "      <td>0.139935</td>\n",
       "      <td>0.267334</td>\n",
       "      <td>0.238234</td>\n",
       "      <td>0.361388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132059</td>\n",
       "      <td>0.057346</td>\n",
       "      <td>0.293063</td>\n",
       "      <td>0.019054</td>\n",
       "      <td>0.306582</td>\n",
       "      <td>0.308038</td>\n",
       "      <td>0.226540</td>\n",
       "      <td>0.498580</td>\n",
       "      <td>0.218263</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.476394</td>\n",
       "      <td>0.248905</td>\n",
       "      <td>0.211974</td>\n",
       "      <td>0.158863</td>\n",
       "      <td>0.441209</td>\n",
       "      <td>0.102294</td>\n",
       "      <td>0.340461</td>\n",
       "      <td>0.690449</td>\n",
       "      <td>0.283526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161760</td>\n",
       "      <td>0.127455</td>\n",
       "      <td>0.198235</td>\n",
       "      <td>0.009023</td>\n",
       "      <td>0.686563</td>\n",
       "      <td>0.278765</td>\n",
       "      <td>0.309352</td>\n",
       "      <td>0.308856</td>\n",
       "      <td>0.383482</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.517651</td>\n",
       "      <td>0.433614</td>\n",
       "      <td>0.233302</td>\n",
       "      <td>0.209073</td>\n",
       "      <td>0.133007</td>\n",
       "      <td>0.107224</td>\n",
       "      <td>0.147974</td>\n",
       "      <td>0.373836</td>\n",
       "      <td>0.123473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056171</td>\n",
       "      <td>0.120732</td>\n",
       "      <td>0.336822</td>\n",
       "      <td>0.304127</td>\n",
       "      <td>0.645655</td>\n",
       "      <td>0.073785</td>\n",
       "      <td>0.738751</td>\n",
       "      <td>0.445481</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.297018</td>\n",
       "      <td>0.633656</td>\n",
       "      <td>0.120866</td>\n",
       "      <td>0.175571</td>\n",
       "      <td>0.713711</td>\n",
       "      <td>0.115258</td>\n",
       "      <td>0.225075</td>\n",
       "      <td>0.234695</td>\n",
       "      <td>0.168673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650333</td>\n",
       "      <td>0.045064</td>\n",
       "      <td>0.222287</td>\n",
       "      <td>0.020970</td>\n",
       "      <td>0.064798</td>\n",
       "      <td>0.928888</td>\n",
       "      <td>0.300316</td>\n",
       "      <td>1.226311</td>\n",
       "      <td>0.180375</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.196958</td>\n",
       "      <td>0.213349</td>\n",
       "      <td>0.125813</td>\n",
       "      <td>0.298986</td>\n",
       "      <td>0.249420</td>\n",
       "      <td>0.054057</td>\n",
       "      <td>0.124869</td>\n",
       "      <td>0.016750</td>\n",
       "      <td>0.265354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141979</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>0.569630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541681</td>\n",
       "      <td>0.865564</td>\n",
       "      <td>0.713655</td>\n",
       "      <td>0.387802</td>\n",
       "      <td>0.060357</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.295249</td>\n",
       "      <td>0.065645</td>\n",
       "      <td>0.027599</td>\n",
       "      <td>0.138417</td>\n",
       "      <td>0.192126</td>\n",
       "      <td>0.085947</td>\n",
       "      <td>0.248956</td>\n",
       "      <td>0.120341</td>\n",
       "      <td>0.209981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182894</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.368058</td>\n",
       "      <td>0.067057</td>\n",
       "      <td>0.084408</td>\n",
       "      <td>0.226232</td>\n",
       "      <td>0.691552</td>\n",
       "      <td>0.532806</td>\n",
       "      <td>0.250970</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.363920</td>\n",
       "      <td>0.363073</td>\n",
       "      <td>0.224018</td>\n",
       "      <td>0.156923</td>\n",
       "      <td>0.525701</td>\n",
       "      <td>0.113057</td>\n",
       "      <td>0.660101</td>\n",
       "      <td>0.323029</td>\n",
       "      <td>0.208863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103917</td>\n",
       "      <td>0.081816</td>\n",
       "      <td>0.280101</td>\n",
       "      <td>0.073121</td>\n",
       "      <td>0.671707</td>\n",
       "      <td>0.327179</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.115005</td>\n",
       "      <td>0.114055</td>\n",
       "      <td>ibizan_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10192</th>\n",
       "      <td>10192</td>\n",
       "      <td>0.265850</td>\n",
       "      <td>0.462945</td>\n",
       "      <td>0.115764</td>\n",
       "      <td>0.076472</td>\n",
       "      <td>0.210322</td>\n",
       "      <td>0.222646</td>\n",
       "      <td>0.393007</td>\n",
       "      <td>0.232940</td>\n",
       "      <td>0.074231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319075</td>\n",
       "      <td>0.090173</td>\n",
       "      <td>0.327772</td>\n",
       "      <td>0.062549</td>\n",
       "      <td>0.132634</td>\n",
       "      <td>0.107856</td>\n",
       "      <td>0.372237</td>\n",
       "      <td>0.371951</td>\n",
       "      <td>0.362590</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>10193</td>\n",
       "      <td>0.055048</td>\n",
       "      <td>0.337380</td>\n",
       "      <td>0.056666</td>\n",
       "      <td>0.143747</td>\n",
       "      <td>0.533744</td>\n",
       "      <td>0.402310</td>\n",
       "      <td>0.313918</td>\n",
       "      <td>0.260754</td>\n",
       "      <td>0.265259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036578</td>\n",
       "      <td>0.270265</td>\n",
       "      <td>0.134933</td>\n",
       "      <td>0.070528</td>\n",
       "      <td>0.393361</td>\n",
       "      <td>0.345819</td>\n",
       "      <td>0.781011</td>\n",
       "      <td>0.131594</td>\n",
       "      <td>0.188321</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10194</th>\n",
       "      <td>10194</td>\n",
       "      <td>0.108173</td>\n",
       "      <td>0.488245</td>\n",
       "      <td>0.127984</td>\n",
       "      <td>0.009157</td>\n",
       "      <td>0.750255</td>\n",
       "      <td>0.145876</td>\n",
       "      <td>0.533477</td>\n",
       "      <td>0.158486</td>\n",
       "      <td>0.291898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125192</td>\n",
       "      <td>0.045357</td>\n",
       "      <td>0.384490</td>\n",
       "      <td>0.119183</td>\n",
       "      <td>0.278313</td>\n",
       "      <td>0.226522</td>\n",
       "      <td>0.128770</td>\n",
       "      <td>0.394680</td>\n",
       "      <td>0.123542</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>10195</td>\n",
       "      <td>0.199903</td>\n",
       "      <td>0.093905</td>\n",
       "      <td>0.194279</td>\n",
       "      <td>0.292338</td>\n",
       "      <td>0.388857</td>\n",
       "      <td>0.368488</td>\n",
       "      <td>0.409774</td>\n",
       "      <td>0.117760</td>\n",
       "      <td>0.111797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181707</td>\n",
       "      <td>0.113832</td>\n",
       "      <td>0.137539</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.278601</td>\n",
       "      <td>0.667906</td>\n",
       "      <td>0.075380</td>\n",
       "      <td>0.049009</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>10196</td>\n",
       "      <td>0.252287</td>\n",
       "      <td>0.173202</td>\n",
       "      <td>0.051504</td>\n",
       "      <td>0.294148</td>\n",
       "      <td>0.317782</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.720302</td>\n",
       "      <td>0.076573</td>\n",
       "      <td>0.287454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045829</td>\n",
       "      <td>0.349389</td>\n",
       "      <td>0.025693</td>\n",
       "      <td>0.652697</td>\n",
       "      <td>0.133479</td>\n",
       "      <td>0.208848</td>\n",
       "      <td>0.006553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>10197</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.449987</td>\n",
       "      <td>0.037762</td>\n",
       "      <td>0.062822</td>\n",
       "      <td>0.202025</td>\n",
       "      <td>0.113575</td>\n",
       "      <td>0.156883</td>\n",
       "      <td>0.152634</td>\n",
       "      <td>0.358045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351387</td>\n",
       "      <td>0.231684</td>\n",
       "      <td>0.161324</td>\n",
       "      <td>0.033184</td>\n",
       "      <td>0.210703</td>\n",
       "      <td>0.766268</td>\n",
       "      <td>0.212991</td>\n",
       "      <td>0.376105</td>\n",
       "      <td>0.061120</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>10198</td>\n",
       "      <td>0.296139</td>\n",
       "      <td>0.314189</td>\n",
       "      <td>0.078163</td>\n",
       "      <td>0.303847</td>\n",
       "      <td>0.155631</td>\n",
       "      <td>0.223437</td>\n",
       "      <td>0.043085</td>\n",
       "      <td>0.146280</td>\n",
       "      <td>0.589337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140868</td>\n",
       "      <td>0.140290</td>\n",
       "      <td>0.116313</td>\n",
       "      <td>0.157629</td>\n",
       "      <td>0.068324</td>\n",
       "      <td>0.154503</td>\n",
       "      <td>0.696577</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.119132</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>10199</td>\n",
       "      <td>0.122369</td>\n",
       "      <td>0.327093</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.131155</td>\n",
       "      <td>0.066748</td>\n",
       "      <td>0.159369</td>\n",
       "      <td>0.036728</td>\n",
       "      <td>0.135146</td>\n",
       "      <td>0.412307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177488</td>\n",
       "      <td>0.560218</td>\n",
       "      <td>0.142714</td>\n",
       "      <td>0.051050</td>\n",
       "      <td>0.337740</td>\n",
       "      <td>0.230842</td>\n",
       "      <td>0.421959</td>\n",
       "      <td>0.179973</td>\n",
       "      <td>0.063298</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10200</th>\n",
       "      <td>10200</td>\n",
       "      <td>0.155863</td>\n",
       "      <td>0.197089</td>\n",
       "      <td>0.068501</td>\n",
       "      <td>0.139961</td>\n",
       "      <td>0.353452</td>\n",
       "      <td>0.221430</td>\n",
       "      <td>0.190142</td>\n",
       "      <td>0.041484</td>\n",
       "      <td>0.269689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333653</td>\n",
       "      <td>0.057023</td>\n",
       "      <td>0.204414</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.082951</td>\n",
       "      <td>0.251341</td>\n",
       "      <td>0.369740</td>\n",
       "      <td>0.349021</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10201</th>\n",
       "      <td>10201</td>\n",
       "      <td>0.107427</td>\n",
       "      <td>0.443477</td>\n",
       "      <td>0.213591</td>\n",
       "      <td>0.200319</td>\n",
       "      <td>0.348853</td>\n",
       "      <td>0.206665</td>\n",
       "      <td>0.538011</td>\n",
       "      <td>0.056137</td>\n",
       "      <td>0.094213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.146225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465748</td>\n",
       "      <td>0.190203</td>\n",
       "      <td>0.747158</td>\n",
       "      <td>0.637603</td>\n",
       "      <td>0.035688</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>10202</td>\n",
       "      <td>0.039015</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.020852</td>\n",
       "      <td>0.064449</td>\n",
       "      <td>0.803862</td>\n",
       "      <td>0.158343</td>\n",
       "      <td>0.079717</td>\n",
       "      <td>0.111955</td>\n",
       "      <td>0.233993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184973</td>\n",
       "      <td>0.213956</td>\n",
       "      <td>0.465987</td>\n",
       "      <td>0.028893</td>\n",
       "      <td>0.396266</td>\n",
       "      <td>0.046148</td>\n",
       "      <td>0.617349</td>\n",
       "      <td>0.063882</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10203</th>\n",
       "      <td>10203</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>1.726596</td>\n",
       "      <td>0.065480</td>\n",
       "      <td>0.244894</td>\n",
       "      <td>0.957199</td>\n",
       "      <td>0.199409</td>\n",
       "      <td>0.490058</td>\n",
       "      <td>0.291278</td>\n",
       "      <td>0.066143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>0.658690</td>\n",
       "      <td>0.045988</td>\n",
       "      <td>0.712698</td>\n",
       "      <td>0.115591</td>\n",
       "      <td>0.306152</td>\n",
       "      <td>0.317170</td>\n",
       "      <td>0.360025</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>10204</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>1.710954</td>\n",
       "      <td>0.202805</td>\n",
       "      <td>0.062407</td>\n",
       "      <td>1.015042</td>\n",
       "      <td>0.293982</td>\n",
       "      <td>0.199711</td>\n",
       "      <td>0.646316</td>\n",
       "      <td>0.126097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.180999</td>\n",
       "      <td>0.281598</td>\n",
       "      <td>0.053496</td>\n",
       "      <td>0.137475</td>\n",
       "      <td>0.311844</td>\n",
       "      <td>0.708423</td>\n",
       "      <td>0.454187</td>\n",
       "      <td>0.119189</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>10205</td>\n",
       "      <td>0.213077</td>\n",
       "      <td>0.464943</td>\n",
       "      <td>0.076155</td>\n",
       "      <td>0.247454</td>\n",
       "      <td>0.175785</td>\n",
       "      <td>0.207633</td>\n",
       "      <td>0.143221</td>\n",
       "      <td>0.013472</td>\n",
       "      <td>0.119089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252777</td>\n",
       "      <td>0.026024</td>\n",
       "      <td>0.079214</td>\n",
       "      <td>0.012825</td>\n",
       "      <td>0.090821</td>\n",
       "      <td>0.308635</td>\n",
       "      <td>0.570012</td>\n",
       "      <td>0.317638</td>\n",
       "      <td>0.260944</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10206</th>\n",
       "      <td>10206</td>\n",
       "      <td>0.052357</td>\n",
       "      <td>0.203870</td>\n",
       "      <td>0.024805</td>\n",
       "      <td>0.048113</td>\n",
       "      <td>0.187230</td>\n",
       "      <td>0.122863</td>\n",
       "      <td>0.215552</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>0.251236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039387</td>\n",
       "      <td>0.029624</td>\n",
       "      <td>0.203387</td>\n",
       "      <td>0.023307</td>\n",
       "      <td>0.125606</td>\n",
       "      <td>0.097455</td>\n",
       "      <td>0.136537</td>\n",
       "      <td>0.046733</td>\n",
       "      <td>0.106547</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>10207</td>\n",
       "      <td>0.056953</td>\n",
       "      <td>0.086293</td>\n",
       "      <td>0.148090</td>\n",
       "      <td>0.023697</td>\n",
       "      <td>0.223520</td>\n",
       "      <td>0.488959</td>\n",
       "      <td>0.057023</td>\n",
       "      <td>0.405565</td>\n",
       "      <td>0.313232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102546</td>\n",
       "      <td>0.383864</td>\n",
       "      <td>0.064658</td>\n",
       "      <td>0.240363</td>\n",
       "      <td>0.812599</td>\n",
       "      <td>0.510581</td>\n",
       "      <td>0.408424</td>\n",
       "      <td>0.626349</td>\n",
       "      <td>0.271534</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>10208</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.258060</td>\n",
       "      <td>0.087704</td>\n",
       "      <td>0.074168</td>\n",
       "      <td>0.360629</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>0.292564</td>\n",
       "      <td>0.278715</td>\n",
       "      <td>0.193194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049158</td>\n",
       "      <td>0.226404</td>\n",
       "      <td>0.438232</td>\n",
       "      <td>0.088425</td>\n",
       "      <td>0.424190</td>\n",
       "      <td>0.420033</td>\n",
       "      <td>0.351746</td>\n",
       "      <td>0.186938</td>\n",
       "      <td>0.156091</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10209</th>\n",
       "      <td>10209</td>\n",
       "      <td>0.096699</td>\n",
       "      <td>0.230751</td>\n",
       "      <td>0.036331</td>\n",
       "      <td>0.239355</td>\n",
       "      <td>0.402590</td>\n",
       "      <td>0.194670</td>\n",
       "      <td>0.430546</td>\n",
       "      <td>0.039796</td>\n",
       "      <td>0.205992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075952</td>\n",
       "      <td>0.127430</td>\n",
       "      <td>0.146586</td>\n",
       "      <td>0.024174</td>\n",
       "      <td>0.054570</td>\n",
       "      <td>0.683321</td>\n",
       "      <td>0.315502</td>\n",
       "      <td>0.028377</td>\n",
       "      <td>0.127600</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10210</th>\n",
       "      <td>10210</td>\n",
       "      <td>0.064586</td>\n",
       "      <td>0.175078</td>\n",
       "      <td>0.029743</td>\n",
       "      <td>0.192120</td>\n",
       "      <td>0.069674</td>\n",
       "      <td>0.258404</td>\n",
       "      <td>0.459893</td>\n",
       "      <td>0.227819</td>\n",
       "      <td>0.342155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.059977</td>\n",
       "      <td>0.130255</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.707049</td>\n",
       "      <td>0.180627</td>\n",
       "      <td>0.337640</td>\n",
       "      <td>0.083628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10211</th>\n",
       "      <td>10211</td>\n",
       "      <td>0.047275</td>\n",
       "      <td>0.367960</td>\n",
       "      <td>0.116011</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.234738</td>\n",
       "      <td>0.122323</td>\n",
       "      <td>0.173898</td>\n",
       "      <td>0.100202</td>\n",
       "      <td>0.515980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044445</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.144953</td>\n",
       "      <td>0.048273</td>\n",
       "      <td>0.363568</td>\n",
       "      <td>0.538690</td>\n",
       "      <td>0.686463</td>\n",
       "      <td>0.308092</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10212</th>\n",
       "      <td>10212</td>\n",
       "      <td>0.152658</td>\n",
       "      <td>0.145025</td>\n",
       "      <td>0.083017</td>\n",
       "      <td>0.109708</td>\n",
       "      <td>0.238628</td>\n",
       "      <td>0.248483</td>\n",
       "      <td>0.174111</td>\n",
       "      <td>0.155712</td>\n",
       "      <td>0.446078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092811</td>\n",
       "      <td>0.475024</td>\n",
       "      <td>0.338946</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.389251</td>\n",
       "      <td>0.252108</td>\n",
       "      <td>0.931645</td>\n",
       "      <td>0.263576</td>\n",
       "      <td>0.121041</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10213</th>\n",
       "      <td>10213</td>\n",
       "      <td>0.129916</td>\n",
       "      <td>0.454492</td>\n",
       "      <td>0.154565</td>\n",
       "      <td>0.138476</td>\n",
       "      <td>0.450985</td>\n",
       "      <td>0.262446</td>\n",
       "      <td>0.235649</td>\n",
       "      <td>0.232123</td>\n",
       "      <td>0.737858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155412</td>\n",
       "      <td>0.279672</td>\n",
       "      <td>0.090964</td>\n",
       "      <td>0.177732</td>\n",
       "      <td>0.684711</td>\n",
       "      <td>0.110205</td>\n",
       "      <td>0.263035</td>\n",
       "      <td>0.566726</td>\n",
       "      <td>0.372779</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10214</th>\n",
       "      <td>10214</td>\n",
       "      <td>0.013476</td>\n",
       "      <td>0.431385</td>\n",
       "      <td>0.145425</td>\n",
       "      <td>0.076874</td>\n",
       "      <td>0.395512</td>\n",
       "      <td>0.275204</td>\n",
       "      <td>0.184758</td>\n",
       "      <td>0.204272</td>\n",
       "      <td>0.444363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.088190</td>\n",
       "      <td>0.172096</td>\n",
       "      <td>0.174134</td>\n",
       "      <td>0.562620</td>\n",
       "      <td>0.317511</td>\n",
       "      <td>0.535123</td>\n",
       "      <td>0.429802</td>\n",
       "      <td>0.067078</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10215</th>\n",
       "      <td>10215</td>\n",
       "      <td>0.190508</td>\n",
       "      <td>0.274778</td>\n",
       "      <td>0.196656</td>\n",
       "      <td>0.291246</td>\n",
       "      <td>0.313179</td>\n",
       "      <td>0.445022</td>\n",
       "      <td>0.311302</td>\n",
       "      <td>0.197148</td>\n",
       "      <td>0.213490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.250032</td>\n",
       "      <td>0.046312</td>\n",
       "      <td>0.297250</td>\n",
       "      <td>0.640016</td>\n",
       "      <td>0.623226</td>\n",
       "      <td>0.337931</td>\n",
       "      <td>0.534723</td>\n",
       "      <td>0.210822</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10216</th>\n",
       "      <td>10216</td>\n",
       "      <td>0.065671</td>\n",
       "      <td>0.155752</td>\n",
       "      <td>0.186963</td>\n",
       "      <td>0.268676</td>\n",
       "      <td>0.610275</td>\n",
       "      <td>0.155434</td>\n",
       "      <td>0.213365</td>\n",
       "      <td>0.745570</td>\n",
       "      <td>0.195844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102288</td>\n",
       "      <td>0.375964</td>\n",
       "      <td>0.391752</td>\n",
       "      <td>0.123938</td>\n",
       "      <td>0.894526</td>\n",
       "      <td>0.264122</td>\n",
       "      <td>0.806418</td>\n",
       "      <td>0.095834</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10217</th>\n",
       "      <td>10217</td>\n",
       "      <td>0.033552</td>\n",
       "      <td>0.024050</td>\n",
       "      <td>0.059881</td>\n",
       "      <td>0.221912</td>\n",
       "      <td>0.172714</td>\n",
       "      <td>0.058706</td>\n",
       "      <td>0.095389</td>\n",
       "      <td>0.312435</td>\n",
       "      <td>0.204258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055424</td>\n",
       "      <td>0.068334</td>\n",
       "      <td>0.175426</td>\n",
       "      <td>0.028316</td>\n",
       "      <td>0.356946</td>\n",
       "      <td>0.466612</td>\n",
       "      <td>0.495386</td>\n",
       "      <td>0.045775</td>\n",
       "      <td>0.052487</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>10218</td>\n",
       "      <td>0.406602</td>\n",
       "      <td>0.375910</td>\n",
       "      <td>0.045285</td>\n",
       "      <td>0.147332</td>\n",
       "      <td>0.124463</td>\n",
       "      <td>0.308772</td>\n",
       "      <td>0.289734</td>\n",
       "      <td>0.112905</td>\n",
       "      <td>0.279398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245650</td>\n",
       "      <td>0.150898</td>\n",
       "      <td>0.240395</td>\n",
       "      <td>0.294611</td>\n",
       "      <td>0.246708</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.314174</td>\n",
       "      <td>0.159350</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10219</th>\n",
       "      <td>10219</td>\n",
       "      <td>0.295789</td>\n",
       "      <td>0.220668</td>\n",
       "      <td>0.202335</td>\n",
       "      <td>0.255805</td>\n",
       "      <td>0.260128</td>\n",
       "      <td>0.216295</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>0.175921</td>\n",
       "      <td>0.215664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104457</td>\n",
       "      <td>0.100329</td>\n",
       "      <td>0.200249</td>\n",
       "      <td>0.018909</td>\n",
       "      <td>0.461901</td>\n",
       "      <td>0.071190</td>\n",
       "      <td>0.432060</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>0.198071</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10220</th>\n",
       "      <td>10220</td>\n",
       "      <td>0.188867</td>\n",
       "      <td>0.382665</td>\n",
       "      <td>0.062665</td>\n",
       "      <td>0.009883</td>\n",
       "      <td>0.251815</td>\n",
       "      <td>0.194007</td>\n",
       "      <td>0.063822</td>\n",
       "      <td>0.248824</td>\n",
       "      <td>0.160611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070799</td>\n",
       "      <td>0.099508</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.419682</td>\n",
       "      <td>0.060405</td>\n",
       "      <td>0.238009</td>\n",
       "      <td>0.384873</td>\n",
       "      <td>0.200187</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10221</th>\n",
       "      <td>10221</td>\n",
       "      <td>0.191749</td>\n",
       "      <td>0.931048</td>\n",
       "      <td>0.063229</td>\n",
       "      <td>0.029506</td>\n",
       "      <td>0.378287</td>\n",
       "      <td>0.381569</td>\n",
       "      <td>0.509593</td>\n",
       "      <td>0.347193</td>\n",
       "      <td>0.188065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.410694</td>\n",
       "      <td>0.519017</td>\n",
       "      <td>0.030706</td>\n",
       "      <td>0.325795</td>\n",
       "      <td>0.069422</td>\n",
       "      <td>0.386810</td>\n",
       "      <td>0.633806</td>\n",
       "      <td>0.457404</td>\n",
       "      <td>brittany_spaniel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10222 rows × 2050 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0               0  0.268131  0.314545  0.099742  0.368813  0.496304  0.127623   \n",
       "1               1  0.535600  0.428560  0.341043  0.183151  0.419171  0.394426   \n",
       "2               2  0.411610  0.346167  0.244413  0.137211  0.465395  0.180839   \n",
       "3               3  0.388489  0.429210  0.204412  0.227942  0.096363  0.562920   \n",
       "4               4  0.336401  0.209270  0.218003  0.294768  0.138025  0.080287   \n",
       "5               5  0.179261  0.179760  0.133865  0.224384  0.227002  0.330131   \n",
       "6               6  0.241880  0.165525  0.069135  0.424274  0.231150  0.137054   \n",
       "7               7  0.142026  0.348606  0.247162  0.341521  0.283595  0.144450   \n",
       "8               8  0.114619  1.468325  0.127875  0.124827  0.282348  0.441101   \n",
       "9               9  0.089565  0.288995  0.011433  0.243081  0.275586  0.239540   \n",
       "10             10  0.341505  0.267562  0.064096  0.231461  0.137724  0.104054   \n",
       "11             11  0.250340  0.332290  0.055295  0.226765  0.351287  0.168232   \n",
       "12             12  0.434534  0.325708  0.200167  0.189003  0.144660  0.102106   \n",
       "13             13  0.273670  0.885893  0.194608  0.132516  0.378691  0.115035   \n",
       "14             14  0.137402  0.665896  0.247767  0.368408  0.293185  0.213098   \n",
       "15             15  0.431510  0.605591  0.071247  0.403741  0.445838  0.282891   \n",
       "16             16  0.123874  0.491449  0.049226  0.352269  0.133436  0.058631   \n",
       "17             17  0.405292  0.367609  0.029684  0.350399  0.474873  0.133523   \n",
       "18             18  0.191915  0.805106  0.216228  0.234681  0.425426  0.129702   \n",
       "19             19  0.366920  0.334553  0.146133  0.209513  0.211596  0.136278   \n",
       "20             20  0.124299  0.924571  0.071838  0.283004  0.211197  0.110888   \n",
       "21             21  0.532367  0.294329  0.071301  0.305470  0.483052  0.117039   \n",
       "22             22  0.179497  0.245316  0.090061  0.194826  0.117493  0.125660   \n",
       "23             23  0.324868  0.865574  0.044108  0.205583  0.496671  0.139935   \n",
       "24             24  0.476394  0.248905  0.211974  0.158863  0.441209  0.102294   \n",
       "25             25  0.517651  0.433614  0.233302  0.209073  0.133007  0.107224   \n",
       "26             26  0.297018  0.633656  0.120866  0.175571  0.713711  0.115258   \n",
       "27             27  0.196958  0.213349  0.125813  0.298986  0.249420  0.054057   \n",
       "28             28  0.295249  0.065645  0.027599  0.138417  0.192126  0.085947   \n",
       "29             29  0.363920  0.363073  0.224018  0.156923  0.525701  0.113057   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "10192       10192  0.265850  0.462945  0.115764  0.076472  0.210322  0.222646   \n",
       "10193       10193  0.055048  0.337380  0.056666  0.143747  0.533744  0.402310   \n",
       "10194       10194  0.108173  0.488245  0.127984  0.009157  0.750255  0.145876   \n",
       "10195       10195  0.199903  0.093905  0.194279  0.292338  0.388857  0.368488   \n",
       "10196       10196  0.252287  0.173202  0.051504  0.294148  0.317782  0.072034   \n",
       "10197       10197  0.028159  0.449987  0.037762  0.062822  0.202025  0.113575   \n",
       "10198       10198  0.296139  0.314189  0.078163  0.303847  0.155631  0.223437   \n",
       "10199       10199  0.122369  0.327093  0.072917  0.131155  0.066748  0.159369   \n",
       "10200       10200  0.155863  0.197089  0.068501  0.139961  0.353452  0.221430   \n",
       "10201       10201  0.107427  0.443477  0.213591  0.200319  0.348853  0.206665   \n",
       "10202       10202  0.039015  0.516958  0.020852  0.064449  0.803862  0.158343   \n",
       "10203       10203  0.117100  1.726596  0.065480  0.244894  0.957199  0.199409   \n",
       "10204       10204  0.033789  1.710954  0.202805  0.062407  1.015042  0.293982   \n",
       "10205       10205  0.213077  0.464943  0.076155  0.247454  0.175785  0.207633   \n",
       "10206       10206  0.052357  0.203870  0.024805  0.048113  0.187230  0.122863   \n",
       "10207       10207  0.056953  0.086293  0.148090  0.023697  0.223520  0.488959   \n",
       "10208       10208  0.064250  0.258060  0.087704  0.074168  0.360629  0.220556   \n",
       "10209       10209  0.096699  0.230751  0.036331  0.239355  0.402590  0.194670   \n",
       "10210       10210  0.064586  0.175078  0.029743  0.192120  0.069674  0.258404   \n",
       "10211       10211  0.047275  0.367960  0.116011  0.016355  0.234738  0.122323   \n",
       "10212       10212  0.152658  0.145025  0.083017  0.109708  0.238628  0.248483   \n",
       "10213       10213  0.129916  0.454492  0.154565  0.138476  0.450985  0.262446   \n",
       "10214       10214  0.013476  0.431385  0.145425  0.076874  0.395512  0.275204   \n",
       "10215       10215  0.190508  0.274778  0.196656  0.291246  0.313179  0.445022   \n",
       "10216       10216  0.065671  0.155752  0.186963  0.268676  0.610275  0.155434   \n",
       "10217       10217  0.033552  0.024050  0.059881  0.221912  0.172714  0.058706   \n",
       "10218       10218  0.406602  0.375910  0.045285  0.147332  0.124463  0.308772   \n",
       "10219       10219  0.295789  0.220668  0.202335  0.255805  0.260128  0.216295   \n",
       "10220       10220  0.188867  0.382665  0.062665  0.009883  0.251815  0.194007   \n",
       "10221       10221  0.191749  0.931048  0.063229  0.029506  0.378287  0.381569   \n",
       "\n",
       "              6         7         8        ...             2039      2040  \\\n",
       "0      0.197701  0.242251  0.240683        ...         0.313441  0.051260   \n",
       "1      0.647834  0.458548  0.191399        ...         0.409865  0.380054   \n",
       "2      0.324190  0.394368  0.409295        ...         0.368454  0.553193   \n",
       "3      0.308009  0.209543  0.299920        ...         0.545821  0.413907   \n",
       "4      0.067682  0.252708  0.529406        ...         0.393116  0.055445   \n",
       "5      0.257557  0.125157  0.399683        ...         0.301558  0.029274   \n",
       "6      0.103857  0.036691  0.063510        ...         0.085624  0.496469   \n",
       "7      0.137762  0.188331  0.476523        ...         0.089191  0.010731   \n",
       "8      0.761734  0.304396  0.313244        ...         0.379911  0.006663   \n",
       "9      0.096828  0.040528  0.026333        ...         0.104241  0.035879   \n",
       "10     0.192591  0.099019  0.279889        ...         0.266945  0.133081   \n",
       "11     0.318178  0.135355  0.320461        ...         0.675125  0.067552   \n",
       "12     0.136878  0.183860  0.193843        ...         0.057799  0.023907   \n",
       "13     0.126360  0.210387  0.287135        ...         0.357561  0.036834   \n",
       "14     0.238626  0.131047  0.387308        ...         0.506970  0.458128   \n",
       "15     0.546789  0.397571  0.282477        ...         1.295817  0.185884   \n",
       "16     0.106053  0.066626  0.209183        ...         0.370686  0.318205   \n",
       "17     0.130907  0.082045  0.338498        ...         0.598368  0.068239   \n",
       "18     0.412327  0.274994  0.201805        ...         0.014675  0.027381   \n",
       "19     0.099392  0.030709  0.234046        ...         0.002897  0.032111   \n",
       "20     0.270058  0.128013  0.206459        ...         0.205755  0.102496   \n",
       "21     0.208858  0.099556  0.181566        ...         0.634013  0.292816   \n",
       "22     0.046893  0.194317  0.530851        ...         0.347062  0.069267   \n",
       "23     0.267334  0.238234  0.361388        ...         0.132059  0.057346   \n",
       "24     0.340461  0.690449  0.283526        ...         0.161760  0.127455   \n",
       "25     0.147974  0.373836  0.123473        ...         0.000000  0.056171   \n",
       "26     0.225075  0.234695  0.168673        ...         0.650333  0.045064   \n",
       "27     0.124869  0.016750  0.265354        ...         0.141979  0.024902   \n",
       "28     0.248956  0.120341  0.209981        ...         0.182894  0.004591   \n",
       "29     0.660101  0.323029  0.208863        ...         0.103917  0.081816   \n",
       "...         ...       ...       ...        ...              ...       ...   \n",
       "10192  0.393007  0.232940  0.074231        ...         0.319075  0.090173   \n",
       "10193  0.313918  0.260754  0.265259        ...         0.036578  0.270265   \n",
       "10194  0.533477  0.158486  0.291898        ...         0.125192  0.045357   \n",
       "10195  0.409774  0.117760  0.111797        ...         0.181707  0.113832   \n",
       "10196  0.720302  0.076573  0.287454        ...         0.000000  0.045829   \n",
       "10197  0.156883  0.152634  0.358045        ...         0.351387  0.231684   \n",
       "10198  0.043085  0.146280  0.589337        ...         0.140868  0.140290   \n",
       "10199  0.036728  0.135146  0.412307        ...         0.177488  0.560218   \n",
       "10200  0.190142  0.041484  0.269689        ...         0.333653  0.057023   \n",
       "10201  0.538011  0.056137  0.094213        ...         0.001037  0.032946   \n",
       "10202  0.079717  0.111955  0.233993        ...         0.184973  0.213956   \n",
       "10203  0.490058  0.291278  0.066143        ...         0.024221  0.038571   \n",
       "10204  0.199711  0.646316  0.126097        ...         0.000331  0.180999   \n",
       "10205  0.143221  0.013472  0.119089        ...         0.252777  0.026024   \n",
       "10206  0.215552  0.012946  0.251236        ...         0.039387  0.029624   \n",
       "10207  0.057023  0.405565  0.313232        ...         0.102546  0.383864   \n",
       "10208  0.292564  0.278715  0.193194        ...         0.049158  0.226404   \n",
       "10209  0.430546  0.039796  0.205992        ...         0.075952  0.127430   \n",
       "10210  0.459893  0.227819  0.342155        ...         0.009302  0.059977   \n",
       "10211  0.173898  0.100202  0.515980        ...         0.044445  0.004374   \n",
       "10212  0.174111  0.155712  0.446078        ...         0.092811  0.475024   \n",
       "10213  0.235649  0.232123  0.737858        ...         0.155412  0.279672   \n",
       "10214  0.184758  0.204272  0.444363        ...         0.004544  0.088190   \n",
       "10215  0.311302  0.197148  0.213490        ...         0.005604  0.250032   \n",
       "10216  0.213365  0.745570  0.195844        ...         0.102288  0.375964   \n",
       "10217  0.095389  0.312435  0.204258        ...         0.055424  0.068334   \n",
       "10218  0.289734  0.112905  0.279398        ...         0.245650  0.150898   \n",
       "10219  0.329740  0.175921  0.215664        ...         0.104457  0.100329   \n",
       "10220  0.063822  0.248824  0.160611        ...         0.000000  0.070799   \n",
       "10221  0.509593  0.347193  0.188065        ...         0.055249  0.410694   \n",
       "\n",
       "           2041      2042      2043      2044      2045      2046      2047  \\\n",
       "0      0.339381  0.129900  0.036992  0.464850  0.404165  0.896529  0.006612   \n",
       "1      0.102255  0.192480  0.321079  0.721681  0.238019  0.487049  0.145839   \n",
       "2      1.081795  0.272835  0.298575  1.154528  0.635267  0.131573  0.061658   \n",
       "3      0.363741  0.131718  0.365889  0.248360  0.077544  0.995600  0.663523   \n",
       "4      0.051030  0.041980  0.222905  0.941616  0.091477  0.985495  0.258923   \n",
       "5      0.232163  0.019113  0.798135  0.920266  0.678088  0.206816  0.019024   \n",
       "6      0.406844  0.368322  0.115259  0.570887  0.789333  0.347292  0.024614   \n",
       "7      0.272730  0.013624  0.086470  1.165994  0.387214  0.908632  0.079460   \n",
       "8      0.382955  0.295957  0.004554  0.334304  0.409749  0.943929  0.087351   \n",
       "9      0.646680  0.307638  0.445135  0.616339  0.508509  0.543701  0.000126   \n",
       "10     0.274200  0.094724  0.054535  0.528031  0.725547  0.168303  0.009646   \n",
       "11     0.408428  0.306361  0.019928  0.695267  0.128020  0.784032  0.005161   \n",
       "12     0.090878  0.094077  0.110228  0.978029  0.549964  0.171643  0.156570   \n",
       "13     0.578533  0.001004  0.014387  0.025846  0.431706  0.591010  0.636501   \n",
       "14     0.198881  0.051571  0.548891  0.424909  0.211861  0.934903  0.019817   \n",
       "15     0.283274  0.126573  0.105886  0.612299  0.240265  1.128039  1.200978   \n",
       "16     0.679017  0.548500  0.869347  0.393311  0.440081  0.569622  0.304598   \n",
       "17     0.180261  0.054458  0.131565  0.583949  0.918237  0.775041  0.018151   \n",
       "18     0.189027  0.041245  0.325206  0.783235  0.442924  0.627115  0.024688   \n",
       "19     0.372326  0.102607  0.112899  0.725864  0.803593  0.273651  0.013946   \n",
       "20     0.908816  0.080889  0.300991  0.801323  0.368663  0.626947  0.083503   \n",
       "21     0.151513  0.070826  0.115341  0.796879  0.466404  0.213822  0.285758   \n",
       "22     0.035161  0.052164  0.027800  0.597529  0.189040  0.842388  0.289425   \n",
       "23     0.293063  0.019054  0.306582  0.308038  0.226540  0.498580  0.218263   \n",
       "24     0.198235  0.009023  0.686563  0.278765  0.309352  0.308856  0.383482   \n",
       "25     0.120732  0.336822  0.304127  0.645655  0.073785  0.738751  0.445481   \n",
       "26     0.222287  0.020970  0.064798  0.928888  0.300316  1.226311  0.180375   \n",
       "27     0.569630  0.000000  0.541681  0.865564  0.713655  0.387802  0.060357   \n",
       "28     0.368058  0.067057  0.084408  0.226232  0.691552  0.532806  0.250970   \n",
       "29     0.280101  0.073121  0.671707  0.327179  0.290023  0.115005  0.114055   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10192  0.327772  0.062549  0.132634  0.107856  0.372237  0.371951  0.362590   \n",
       "10193  0.134933  0.070528  0.393361  0.345819  0.781011  0.131594  0.188321   \n",
       "10194  0.384490  0.119183  0.278313  0.226522  0.128770  0.394680  0.123542   \n",
       "10195  0.137539  0.002664  0.377358  0.278601  0.667906  0.075380  0.049009   \n",
       "10196  0.349389  0.025693  0.652697  0.133479  0.208848  0.006553  0.000000   \n",
       "10197  0.161324  0.033184  0.210703  0.766268  0.212991  0.376105  0.061120   \n",
       "10198  0.116313  0.157629  0.068324  0.154503  0.696577  0.308000  0.119132   \n",
       "10199  0.142714  0.051050  0.337740  0.230842  0.421959  0.179973  0.063298   \n",
       "10200  0.204414  0.002921  0.082951  0.251341  0.369740  0.349021  0.002398   \n",
       "10201  0.146225  0.000000  0.465748  0.190203  0.747158  0.637603  0.035688   \n",
       "10202  0.465987  0.028893  0.396266  0.046148  0.617349  0.063882  0.006458   \n",
       "10203  0.658690  0.045988  0.712698  0.115591  0.306152  0.317170  0.360025   \n",
       "10204  0.281598  0.053496  0.137475  0.311844  0.708423  0.454187  0.119189   \n",
       "10205  0.079214  0.012825  0.090821  0.308635  0.570012  0.317638  0.260944   \n",
       "10206  0.203387  0.023307  0.125606  0.097455  0.136537  0.046733  0.106547   \n",
       "10207  0.064658  0.240363  0.812599  0.510581  0.408424  0.626349  0.271534   \n",
       "10208  0.438232  0.088425  0.424190  0.420033  0.351746  0.186938  0.156091   \n",
       "10209  0.146586  0.024174  0.054570  0.683321  0.315502  0.028377  0.127600   \n",
       "10210  0.130255  0.007854  0.707049  0.180627  0.337640  0.083628  0.000000   \n",
       "10211  0.144953  0.048273  0.363568  0.538690  0.686463  0.308092  0.455800   \n",
       "10212  0.338946  0.004881  0.389251  0.252108  0.931645  0.263576  0.121041   \n",
       "10213  0.090964  0.177732  0.684711  0.110205  0.263035  0.566726  0.372779   \n",
       "10214  0.172096  0.174134  0.562620  0.317511  0.535123  0.429802  0.067078   \n",
       "10215  0.046312  0.297250  0.640016  0.623226  0.337931  0.534723  0.210822   \n",
       "10216  0.391752  0.123938  0.894526  0.264122  0.806418  0.095834  0.000526   \n",
       "10217  0.175426  0.028316  0.356946  0.466612  0.495386  0.045775  0.052487   \n",
       "10218  0.240395  0.294611  0.246708  0.130435  0.314174  0.159350  0.005721   \n",
       "10219  0.200249  0.018909  0.461901  0.071190  0.432060  0.044959  0.198071   \n",
       "10220  0.099508  0.003047  0.419682  0.060405  0.238009  0.384873  0.200187   \n",
       "10221  0.519017  0.030706  0.325795  0.069422  0.386810  0.633806  0.457404   \n",
       "\n",
       "                   2048  \n",
       "0          ibizan_hound  \n",
       "1          ibizan_hound  \n",
       "2          ibizan_hound  \n",
       "3          ibizan_hound  \n",
       "4          ibizan_hound  \n",
       "5          ibizan_hound  \n",
       "6          ibizan_hound  \n",
       "7          ibizan_hound  \n",
       "8          ibizan_hound  \n",
       "9          ibizan_hound  \n",
       "10         ibizan_hound  \n",
       "11         ibizan_hound  \n",
       "12         ibizan_hound  \n",
       "13         ibizan_hound  \n",
       "14         ibizan_hound  \n",
       "15         ibizan_hound  \n",
       "16         ibizan_hound  \n",
       "17         ibizan_hound  \n",
       "18         ibizan_hound  \n",
       "19         ibizan_hound  \n",
       "20         ibizan_hound  \n",
       "21         ibizan_hound  \n",
       "22         ibizan_hound  \n",
       "23         ibizan_hound  \n",
       "24         ibizan_hound  \n",
       "25         ibizan_hound  \n",
       "26         ibizan_hound  \n",
       "27         ibizan_hound  \n",
       "28         ibizan_hound  \n",
       "29         ibizan_hound  \n",
       "...                 ...  \n",
       "10192  brittany_spaniel  \n",
       "10193  brittany_spaniel  \n",
       "10194  brittany_spaniel  \n",
       "10195  brittany_spaniel  \n",
       "10196  brittany_spaniel  \n",
       "10197  brittany_spaniel  \n",
       "10198  brittany_spaniel  \n",
       "10199  brittany_spaniel  \n",
       "10200  brittany_spaniel  \n",
       "10201  brittany_spaniel  \n",
       "10202  brittany_spaniel  \n",
       "10203  brittany_spaniel  \n",
       "10204  brittany_spaniel  \n",
       "10205  brittany_spaniel  \n",
       "10206  brittany_spaniel  \n",
       "10207  brittany_spaniel  \n",
       "10208  brittany_spaniel  \n",
       "10209  brittany_spaniel  \n",
       "10210  brittany_spaniel  \n",
       "10211  brittany_spaniel  \n",
       "10212  brittany_spaniel  \n",
       "10213  brittany_spaniel  \n",
       "10214  brittany_spaniel  \n",
       "10215  brittany_spaniel  \n",
       "10216  brittany_spaniel  \n",
       "10217  brittany_spaniel  \n",
       "10218  brittany_spaniel  \n",
       "10219  brittany_spaniel  \n",
       "10220  brittany_spaniel  \n",
       "10221  brittany_spaniel  \n",
       "\n",
       "[10222 rows x 2050 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_mix.drop(data_mix.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 2049)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.asarray(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 2049)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(features[:,0:2048], features[:,-1],train_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7155, 2048)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(features, target):\n",
    "    \"\"\"\n",
    "    To train the random forest classifier with features and target data\n",
    "    :param features:\n",
    "    :param target:\n",
    "    :return: trained random forest classifier\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(features, target)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y= le.transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 63,  60, 115, ...,  74,  92,   9])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y= le.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model ::  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Actual outcome :: 54 and Predicted outcome :: 54\n",
      "Actual outcome :: 100 and Predicted outcome :: 100\n",
      "Actual outcome :: 97 and Predicted outcome :: 97\n",
      "Actual outcome :: 77 and Predicted outcome :: 106\n",
      "Actual outcome :: 61 and Predicted outcome :: 84\n",
      "Train Accuracy ::  0.9973091976516634\n",
      "Test Accuracy  ::  0.7872513857189436\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create random forest classifier instance\n",
    "    trained_model = random_forest_classifier(train_x, train_Y)\n",
    "    print \"Trained model :: \", trained_model\n",
    "    predictions = trained_model.predict(test_x)\n",
    "    \n",
    "    for i in xrange(0, 5):\n",
    "        print \"Actual outcome :: {} and Predicted outcome :: {}\".format(list(test_Y)[i], predictions[i])\n",
    "        \n",
    "    print \"Train Accuracy :: \", accuracy_score(train_Y, trained_model.predict(train_x))\n",
    "    print \"Test Accuracy  :: \", accuracy_score(test_Y, predictions) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(features[:,0:2048], features[:,-1],train_size=0.7)\n",
    "train_Y= le.transform(train_y)\n",
    "test_Y= le.transform(test_y)           \n",
    "\n",
    "########## Tried increasing the training dataset  #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model ::  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Actual outcome :: 82 and Predicted outcome :: 82\n",
      "Actual outcome :: 88 and Predicted outcome :: 88\n",
      "Actual outcome :: 22 and Predicted outcome :: 22\n",
      "Actual outcome :: 101 and Predicted outcome :: 43\n",
      "Actual outcome :: 4 and Predicted outcome :: 104\n",
      "Train Accuracy ::  0.9970649895178197\n",
      "Test Accuracy  ::  0.8089338115422237\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create random forest classifier instance\n",
    "    trained_model = random_forest_classifier(train_x, train_Y)\n",
    "    print \"Trained model :: \", trained_model\n",
    "    predictions = trained_model.predict(test_x)\n",
    "    \n",
    "    for i in xrange(0, 5):\n",
    "        print \"Actual outcome :: {} and Predicted outcome :: {}\".format(list(test_Y)[i], predictions[i])\n",
    "        \n",
    "    print \"Train Accuracy :: \", accuracy_score(train_Y, trained_model.predict(train_x))\n",
    "    print \"Test Accuracy  :: \", accuracy_score(test_Y, predictions) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
